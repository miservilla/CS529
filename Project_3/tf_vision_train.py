# -*- coding: utf-8 -*-
"""TF_vision_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CXKg1eYPLF8UXy9lWTcEpaGH4triSpK5
"""

# Commented out IPython magic to ensure Python compatibility.
# from google.colab import drive
# drive.mount('/content/drive')
# # %cd '/content/drive/MyDrive/Colab Notebooks/CS529'

# !pip install patchify

# import sys
# sys.path.append('/content/drive/MyDrive/Colab Notebooks/CS529')

import os
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"

import numpy as np
import cv2
from glob import glob
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from patchify import patchify
import tensorflow as tf
from vit import ViT
from tensorflow import keras
from keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau, EarlyStopping
import pandas as pd

# Hyperparameters

hp = {}
hp['image_size'] = 200
hp['num_channels'] = 3
hp['patch_size'] = 25
hp['num_patches'] = (hp['image_size']**2) // (hp['patch_size']**2)
hp['flat_patches_shape'] = (hp['num_patches'], 
                            (hp['patch_size']**2)*hp['num_channels'])
hp['batch_size'] = 8
hp['lr'] = 1e-4
hp['num_epochs'] = 35
hp['num_classes'] = 12
hp['class_names'] = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 
                     'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 
                     'Scentless Mayweed', 'Shepherds Purse', 
                     'Small-flowered Cranesbill', 'Sugar beet']

hp['num_layers'] = 12
hp['hidden_dim'] = 768
hp['mlp_dim'] = 3072
hp['num_heads'] = 12
hp['dropout_rate'] = 0.2

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

def load_data(path, split=0.1):
    images = shuffle(glob(os.path.join(path, '*', '*.png')))
    split_size = int(len(images) * split)
    train_x, valid_x = train_test_split(images, test_size=split_size, 
                                        random_state=42)
    train_x, test_x = train_test_split(train_x, test_size=split_size, 
                                        random_state=42)
    return train_x, valid_x, test_x

def process_image_label(path):
    path = path.decode()
    image = cv2.imread(path, cv2.IMREAD_COLOR)
    image = cv2.resize(image, (hp['image_size'], hp['image_size']))
    image = image/255.0
    patch_shape = (hp['patch_size'], hp['patch_size'], hp['num_channels'])
    patches = patchify(image, patch_shape, hp['patch_size'])
    patches = np.reshape(patches, hp['flat_patches_shape'])
    patches = patches.astype(np.float32)
    class_name = path.split('/')[-2]
    class_idx = hp['class_names'].index(class_name)
    class_idx = np.array(class_idx, dtype=np.int32)

    return patches, class_idx

def parse(path):
    patches, labels = tf.numpy_function(process_image_label, [path], 
                                        [tf.float32, tf.int32])
    labels = tf.one_hot(labels, hp['num_classes'])

    patches.set_shape(hp['flat_patches_shape'])
    labels.set_shape(hp['num_classes'])

    return patches, labels

def tf_dataset(images, batch=8):
    ds = tf.data.Dataset.from_tensor_slices((images))
    ds = ds.map(parse).batch(batch).prefetch(8)
    return ds

if __name__ == '__main__':
    """Seeding"""
    np.random.seed(42)
    tf.random.set_seed(42)
    create_dir('./files')
    dataset_path = '/home/michaelservilla/CS529/Project_3/train'
    model_path = os.path.join('files', 'model.h5')
    csv_path = os.path.join('files', 'log.csv')
    train_x, valid_x, test_x = load_data(dataset_path)

    train_ds = tf_dataset(train_x, batch=hp['batch_size'])
    valid_ds = tf_dataset(valid_x, batch=hp['batch_size'])
    test_ds = tf_dataset(test_x, batch=hp['batch_size'])

    model = ViT(hp)
    model.compile(
        loss="categorical_crossentropy",
        optimizer=tf.keras.optimizers.Adam(hp['lr'], clipvalue=1.0),
        metrics=['acc']
    )

    callbacks = [
        ModelCheckpoint(model_path, monitor='val_loss', verbose=1, 
                        save_best_only=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, 
                          min_lr=1e-10, verbose=1),
        CSVLogger(csv_path),
        EarlyStopping(monitor='val_loss', patience=50, 
                      restore_best_weights=False)
    ]

    model.fit(
        train_ds,
        epochs=hp['num_epochs'],
        validation_data=valid_ds,
        callbacks=callbacks
    )

    model.load_weights(model_path)
    model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
      optimizer=tf.keras.optimizers.Adam(hp["lr"]),
      metrics=["acc"]
    )

    eval = model.evaluate(test_ds)

log_df = pd.read_csv(csv_path)
import matplotlib.pyplot as plt

x_axis = log_df['epoch'].to_numpy()
train_error = 1 - log_df['acc'].to_numpy()
validate_error = 1 -  log_df['val_acc'].to_numpy()

plt.plot(x_axis, train_error, marker = 'o', label = "Train Error")
plt.plot(x_axis, validate_error, marker = 'o', label = "Validation Error")
plt.ylim(0,1)
plt.xlim(0,len(log_df))
plt.title("Error Over Epochs")
plt.xlabel("Number of Epochs")
plt.ylabel("Error")
plt.legend()
plt.show()

validate_error

kaggle_dataset_path = '/home/michaelservilla/CS529/Project_3/test'

kaggle_test_x = glob(os.path.join(kaggle_dataset_path,'*.png'))
kaggle_test_ds = tf_dataset(kaggle_test_x, batch=hp['batch_size'])
hp['class_names'] = ['test']

image_names = []

for path in kaggle_test_x:
  image_names.append(path.split('/')[-1])

# print(image_names)

prediction =  model.predict(kaggle_test_ds)

pred_classes = np.argmax(prediction,axis=1)
classes = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 
                     'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 
                     'Scentless Mayweed', 'Shepherds Purse', 
                     'Small-flowered Cranesbill', 'Sugar beet']

# print(pred_classes)

pred_classes = [classes[pred_classes[i]] for i in range(len(pred_classes))]

# print(pred_classes)

kaggle_predictions = pd.DataFrame(list(zip(image_names, pred_classes)),
               columns =['file', 'species'])

print(kaggle_predictions)

kaggle_csv_path = os.path.join('files', 'kaggle_pred.csv')

kaggle_predictions.to_csv(kaggle_csv_path, index=False)